# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15wa0JtexBMI-i8xqmhiZyHQ9Cgx-crGQ
"""

import pandas as pd
import numpy as np
import nltk
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer


data = pd.read_csv('/train.csv')

# Data properties
print(data.columns)
data.shape
data.head()
data.describe()
data.isnull().sum()
data.isnull().any()

# Data Pre-Processing
data_part = data.dropna()
data_part.isnull().sum()

# Data Tokenization

nltk.download('punkt')
sentence = "Welcome CIVE. This stands for College of Informatics and Virtual Education"
tokens = nltk.sent_tokenize(sentence)

tokens_words = nltk.word_tokenize(sentence)
print(tokens)
print(tokens_words)
type(tokens_words)
len(tokens_words)

# Data Stemming
ps = PorterStemmer()
words = nltk.word_tokenize(sentence)
for i in words:
  print(i, ":", ps.stem(i))
new_words = ['like','likes','liked','liking']
for w in new_words:
  print(w, ":", ps.stem(w))

# Data Lemmatization
nltk.download('wordnet')
nltk.download('omw-1.4')
lemmati = WordNetLemmatizer()
other_words = ['socks','sons','daughters']
for o in other_words:
  print(o,":",lemmati.lemmatize(o))

for news in words:
  print(news,":",lemmati.lemmatize(news))

  # Data Stop words
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_word = """ Data science is one of the most trending field to  work with, It needs data to give prediction by using past senarios"""
  #needs tokenization
newStopWord = nltk.word_tokenize(stop_word)
stop_words = set(stopwords.words('english'))
print(stop_words)
print(newStopWord)

for token_word in newStopWord:
  if token_word in stop_words:
    print(token_word)